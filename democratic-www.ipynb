{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3e6135",
   "metadata": {},
   "source": [
    "# A Democratic World Wide Web\n",
    "## An Introduction and Implementation of the Plebeian Algorithm to Freely Combat Misinformation\n",
    "## By: Benjamin D. Fedoruk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134fa182",
   "metadata": {},
   "source": [
    "### About the Speaker\n",
    "Benjamin D. Fedoruk is a data scientist, currently studying mathematics and computer science at Ontario Tech University. He has completed various data science works, including research into carbon pricing efficacy, public transit systems in northern/rural communities, and most majorly, has conducted research to reduce the spread of misinformation on social media. He is interested in NLP, and data science at-large. \n",
    "\n",
    "### Workshop Overview\n",
    "The Internet is filled with a vast array of information -- some is true, some is false. But, is there a programmatic method to effectively determine the verity of information, without the need for human intervention? This is where the Plebeian Algorithm shines, which you will learn about in this workshop. You will learn how to implement a Plebeian-esque algorithm into your personal projects. \n",
    "\n",
    "### Intended Audience\n",
    "This workshop is for you if:\n",
    "\n",
    "- you have an introductory level of data science skills in Python.\n",
    "- you are interested in natural language processing.\n",
    "- you want to learn applications of data science skills to real-world problems.\n",
    "- you want to find solutions to the spread of misinformation. \n",
    "- you are intrigued by problem solving approaches using data science. \n",
    "\n",
    "### Topics Covered\n",
    "Topics covered in this workshop include:\n",
    "\n",
    "- Plebeian algorithms\n",
    "- democratic moderation of content\n",
    "- natural language processing, applied to a real-world issue\n",
    "- expansion on the basics of Python data science. \n",
    "\n",
    "### Workshop Takeaways\n",
    "Topics covered in this workshop include:\n",
    "\n",
    "- Plebeian algorithms\n",
    "- democratic moderation of content\n",
    "- natural language processing, applied to a real-world issue\n",
    "- expansion on the basics of Python data science. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a43fc56",
   "metadata": {},
   "source": [
    "## Pre-Requisites\n",
    "\n",
    "I am going to be using Jupyter Notebook for this course, but you can feel free to follow along in any other Python setup. \n",
    "\n",
    "If you haven't installed some of the core Python packages (using `pip`), here's what you'll need to run (on a Bash prompt). This can also be done using Anaconda, in a similar way. I'm assuming that if you're attending this workshop, you'll know how to install packages for your specific Python setup! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "99e3ce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/ben/.local/lib/python3.9/site-packages (1.21.2)\n",
      "Requirement already satisfied: matplotlib in /home/ben/.local/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: nltk in /home/ben/.local/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: pandas in /home/ben/.local/lib/python3.9/site-packages (1.3.3)\n",
      "Requirement already satisfied: demoji in /home/ben/.local/lib/python3.9/site-packages (1.1.0)\n",
      "Requirement already satisfied: seaborn in /home/ben/.local/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3.9/site-packages (1.26.6)\n",
      "Requirement already satisfied: basc_py4chan in /home/ben/.local/lib/python3.9/site-packages (0.6.5)\n",
      "Requirement already satisfied: bs4 in /home/ben/.local/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: requests>=1.0.0 in /usr/lib/python3.9/site-packages (from basc_py4chan) (2.26.0)\n",
      "Requirement already satisfied: chardet>=3.0.2 in /usr/lib/python3.9/site-packages (from requests>=1.0.0->basc_py4chan) (4.0.0)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/lib/python3.9/site-packages (from requests>=1.0.0->basc_py4chan) (3.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3.9/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3.9/site-packages (from matplotlib) (8.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ben/.local/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ben/.local/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /home/ben/.local/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ben/.local/lib/python3.9/site-packages (from nltk) (2021.10.21)\n",
      "Requirement already satisfied: joblib in /home/ben/.local/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ben/.local/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/ben/.local/lib/python3.9/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Below is the command you should run in a prompt:\n",
    "!pip install numpy matplotlib nltk pandas demoji seaborn urllib3 basc_py4chan bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48bcc8",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Below are the imports for all of the work we'll be doing on this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1bbdc1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c099cfd8",
   "metadata": {},
   "source": [
    "And here are some of the basic declarations we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc197b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71134834",
   "metadata": {},
   "source": [
    "## Analyzing YouTube Comments from a Dataset\n",
    "The first exploration we will do with the Plebeian Algorithm will be an analysis of YouTube comments from a dataset. The dataset should be contained in the same directory as this .ipynb file, saved as a CSV (comma-separated values) file. The file is named `youtube_comments_usa.csv`. \n",
    "\n",
    "Although it may be advisable to add a popularity check to the comments being analyzed, this approach will take all comments into account, as most posts have less than 5 likes or replies. \n",
    "\n",
    "**Warning**: The comments in this dataset are not curated, and are directly gathered from YouTube. As such, there is a strong chance that some comments may contain choice language, or potentially triggering material. I recommend that if this is a concern for you, that you avoid printing out the `comment_text` column of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f9bab",
   "metadata": {},
   "source": [
    "### Import the Dataset\n",
    "Below, we shall import the dataset using pandas. Some lines in the CSV are skipped -- although this is unfortunate, this workshop will not focus on the data cleaning of this retrieved data. Instead, we will simply skip those entries: we have enough data without them. They will be stored in a pandas dataframe, which can essentially be thought of as a table, with rows and columns. The columns have headers, which we will print in the subsequent section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13eec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e174fcd4",
   "metadata": {},
   "source": [
    "### Preliminary Analysis of the Dataset\n",
    "Here's some quick information about the dataset we're working with. I'll print off the first few comments, the columns, and the number of rows/comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b3e13ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links_translate_emoji(text: str) -> str:\n",
    "    return re.sub(r'\\w:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', demoji.replace_with_desc(text, ' ').lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf19998",
   "metadata": {},
   "source": [
    "And here's some of the basics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56101d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b01f2b67",
   "metadata": {},
   "source": [
    "### Data Cleaning and Sentiment Analysis\n",
    "Let's do some quick cleaning and also generate some sentiment analysis for each comment. We are using VADER, as described above. VADER is the ideal sentiment analyzer for informal text with slang (see *Hands-On Python Natural Language Processing* by Kedia and Rasu). Obviously, YouTube comments contain this level of informal language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6d43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cd18f5c",
   "metadata": {},
   "source": [
    "### Visualize The Results\n",
    "Great work so far! We will now use seaborn to visualize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab338ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15d6a895",
   "metadata": {},
   "source": [
    "## Analyzing Reddit using API\n",
    "Now we'll do another case study, using Reddit. We will be using `urllib` and `pushshift` to grab posts on Reddit. This can include original posts and comments, however for this exercise (for the sake of time) I will be only gathering the headlines. Obviously this will be easier to code, but it should be noted that the results may not be useful in any meaningful sense. \n",
    "\n",
    "**Warning**: The text entries gathered in this exercise will be gathered live during the workshop. I cannot guarantee the profanity of the language used. It may be triggering to some individuals. I recommend that if this is a concern for you, that you avoid printing out the text entries gathered. \n",
    "### Pre-Requisites for URLLib\n",
    "Below are some quick setup procedures we need to follow for `urllib`. To save time, I have provided these in the pre-notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8e1de517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(lower_bound_timestamp, upper_bound_timestamp, target_result_size, target_subreddit, score_threshold):\n",
    "    headline_collection = set()\n",
    "    reddit_data_url = f'https://api.pushshift.io/reddit/submission/search/?after={lower_bound_timestamp}&before={upper_bound_timestamp}&sort_type=score&sort=desc&subreddit={target_subreddit}&size={target_result_size}&score={score_threshold}'\n",
    "    \n",
    "    try:\n",
    "        with urllib.request.urlopen(reddit_data_url) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            for submission in data['data']:\n",
    "                headline_collection.add(submission['title'])\n",
    "        return headline_collection\n",
    "    except urllib.error.HTTPError as e:\n",
    "        return set()\n",
    "    except urllib.error.URLError as e:\n",
    "        return set()\n",
    "    \n",
    "headlines = set()\n",
    "time_now = datetime.datetime.now()\n",
    "limit_delta = 365\n",
    "limit_lower_delta = 360\n",
    "subreddit = \"politics\"\n",
    "result_size = 1000\n",
    "score_limit = \">1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460b89e",
   "metadata": {},
   "source": [
    "### Scrape from Reddit\n",
    "We'll now scrape from Reddit using the method we created above. This step may take some time to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8cfd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d1b4d0a",
   "metadata": {},
   "source": [
    "### Get Sentiment Analysis Results\n",
    "You know the drill -- time to do some sentiment analysis using nltk and VADER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a3df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d0292b8",
   "metadata": {},
   "source": [
    "### Visualize the Results\n",
    "Well, that was a little bit more tough than last time, but hopefully it feels a little bit familiar. Hopefully at this point, you're beginning to understand the idea we're going for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980db4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b09635d4",
   "metadata": {},
   "source": [
    "## Analyzing 4chan using API\n",
    "Let's do one final analysis using 4chan. We will be using `basc_py4chan` to grab posts on 4chan. The 4chan platform offers users visibility of only the top posts -- once a post is no longer in the top set of posts, it is removed. Thus, 4chan offers users ephemerality, and also anonymity. \n",
    "\n",
    "**Warning**: The text entries gathered in this exercise will be gathered live during the workshop. I cannot guarantee the profanity of the language used. It may be triggering to some individuals. I recommend that if this is a concern for you, that you avoid printing out the text entries gathered. I know I've been saying this for the past few, but this is the one time I'm going to nearly guarantee that you'll see some potentially disturbing material. As mentioned, this is due to the ephemerality and anonymity of 4chan. I'll avoid printing them out. \n",
    "### Pre-Requisites\n",
    "Below are some pre-requisites for this analysis. I haven't included these herein; I think these will be easy enough to type out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bdf55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2361a9e",
   "metadata": {},
   "source": [
    "### Scrape /b/ from 4chan\n",
    "We are going to be scraping the /b/ board from 4chan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580b546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c5e7916",
   "metadata": {},
   "source": [
    "### Perform Sentiment Analysis on Results\n",
    "You guessed it! Time to do the same process again! Let's use nltk and VADER to analyze the sentiment of these posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0038fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4ba9fa0",
   "metadata": {},
   "source": [
    "### Visualize the Results\n",
    "Time to see what this means, again! Let's plot these results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4317c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
